{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-902dd5d5b571df86",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Exercise 03\n",
    "**Kernel Methods in Machine Learning (CS-E4830)**\n",
    "\n",
    "**Release date**: 12th of March, 2019\n",
    "\n",
    "**Submission date**: 28st of March, 2019 @4PM (no late submission allowed)\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "    <b>Tasks:</b>\n",
    "\n",
    "1. [Support Vector Machines (SVM)](#csvm)\n",
    "  1. [Dual optimization using quadratic programming (QP)](#task_1) (**2 Points**)\n",
    "  2. [Comparison to LibSVM](#task_2) (**1 Point**)\n",
    "  3. [Visualize Support Vectors](#visual_sv)\n",
    "2. [Non-linear Kernels](#non_linear_kernels)\n",
    "  1. [Tanimoto kernel for binary data](#tanimoto_kernel) (**1 Point**)\n",
    "  2. [MinMax kernel for non-negative data](#minmax_kernel) (**1 Point**)\n",
    "</div> \n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "    <b>Bonus Task:</b>\n",
    "    \n",
    "3. [Toxicity Prediction using Non-Linear SVM](#toxicity_prediction) (**1 Bonus-point**)\n",
    "\n",
    "</div>\n",
    "\n",
    "**Version**: 1.2\n",
    "\n",
    "**Version history**:\n",
    "\n",
    "- 1.0: Initial version\n",
    "- 1.1: Modify SVM class: For $|\\alpha_i - C| < \\epsilon$ we set $\\alpha_i = C$. This fix improves the numerical stability of the SVM [bias $b$ estimation](#bias).\n",
    "- 1.2: Minor changes: Fix doc-string of ```decision_function```, rename ```self._ytrain``` to ```self._y_train``` in ```__init__```, and indicate what is the [*bias constraint*](#SVM_dual)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\">\n",
    "    Please add you student number and email address to the notebook into the corresponding cell.\n",
    "</div>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EMAIL: firstname.lastname@aalto.fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STUDENT_NUMBER: 000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-3d7c3a3baf2e8a84",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import scipy.optimize as spopt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, KFold\n",
    "from sklearn.svm import SVC as SVM_sk\n",
    "from sklearn.datasets import make_blobs, make_moons\n",
    "from sklearn.metrics.pairwise import rbf_kernel as rbf_kernel_sk\n",
    "from sklearn.metrics.pairwise import linear_kernel as linear_kernel_sk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_kernel_wrapper(X, Y=None, sigma=None):\n",
    "    \"\"\"\n",
    "    Wrapper around the sklearn rbf-kernel function. It converts between the\n",
    "    gamma parametrization (sklearn) and the sigma parametrization (lecture).\n",
    "    \"\"\"\n",
    "    if sigma is None:\n",
    "        sigma = np.sqrt(X.shape[1] / 2.)\n",
    "\n",
    "    return rbf_kernel_sk(X, Y, gamma=(1. / (2. * (sigma**2))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_svm_model(X, y, svm, ax=None, show_origin=False, verbose=True):\n",
    "    \"\"\"\n",
    "    Helper function to plot svm models for simple 2D-data.\n",
    "    \"\"\"\n",
    "    # Fit model\n",
    "    svm.fit(X, y)\n",
    "    \n",
    "    if verbose:\n",
    "        if isinstance(svm, SVM_sk):\n",
    "            print(\"Number of support vectors:\", svm.n_support_)\n",
    "            print(\"Bias:\", np.round(svm.intercept_, 4))\n",
    "        else:\n",
    "            print(\"Number of support vectors:\", svm.n_sv)\n",
    "            print(\"Bias:\", np.round(svm._bias, 4))\n",
    "            print(\"Dual variables:\\n\", np.round(svm._alpha[svm._alpha > 0], 4))\n",
    "\n",
    "    if ax is None:\n",
    "        fig = plt.figure()\n",
    "        ax = fig.add_subplot(111)\n",
    "        \n",
    "    _ = ax.scatter(X[y == 1, 0], X[y == 1, 1], c=\"blue\", s=50, label=\"positive class\")\n",
    "    _ = ax.scatter(X[y == -1, 0], X[y == -1, 1], c=\"red\", s=50, label=\"negative class\")\n",
    "\n",
    "    # plot the decision function\n",
    "    xlim = ax.get_xlim()\n",
    "    ylim = ax.get_ylim()\n",
    "    \n",
    "    if show_origin:\n",
    "        xlim = (np.minimum(-0.5, xlim[0]), np.maximum(0.5, xlim[1]))\n",
    "        ylim = (np.minimum(-0.5, ylim[0]), np.maximum(0.5, ylim[1]))\n",
    "        \n",
    "        ax.plot(0, 0, 's', c=\"k\", label=\"Origin\")\n",
    "\n",
    "    # create grid to evaluate model\n",
    "    xx = np.linspace(xlim[0], xlim[1], 30)\n",
    "    yy = np.linspace(ylim[0], ylim[1], 30)\n",
    "    YY, XX = np.meshgrid(yy, xx)\n",
    "    xy = np.vstack([XX.ravel(), YY.ravel()]).T\n",
    "    Z = svm.decision_function(xy).reshape(XX.shape)\n",
    "\n",
    "    # plot decision boundary and margins\n",
    "    _ = ax.contour(XX, YY, Z, colors='k', levels=[-1, 0, 1], alpha=0.5, \n",
    "                   linestyles=['--', '-', '--'])\n",
    "    # plot support vectors\n",
    "    _ = ax.scatter(svm.support_vectors_[:, 0], svm.support_vectors_[:, 1], s=200,\n",
    "                   linewidth=1.5, facecolors='none', edgecolors='k', label=\"Support vectors\")\n",
    "    _ = ax.legend()\n",
    "    \n",
    "    _ = ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. C - Support Vector Machine (C-SVM) <a id='csvm'></a>\n",
    "\n",
    "In this task you are going to implement a soft-margin C-SVM. You will use the dual formulation (derived in the Pen & Paper exercise) to find the optimal model using quadratic programming (QP).\n",
    "\n",
    "#### SciPy Optimization Toolbox\n",
    "\n",
    "A convenient interface to a QP-solver is provided by the [scipy.optimization](https://docs.scipy.org/doc/scipy-1.1.0/reference/optimize.html) package (JupyterHub uses version 1.1.0). As optimization algorithm we will use  [Sequential Least SQuares Programming (SLSQP)](https://neos-guide.org/content/sequential-quadratic-programming) (```scipy.optimize.minimize(..., method=\"SLSQP\")```). Another popular framework for optimization in Python is, e.g., [cvxpy](https://www.cvxpy.org/) (not available on JupyterHub).\n",
    "\n",
    "#### SVM Primal formulation\n",
    "\n",
    "For a given training set $\\{(\\mathbf{x}_i,y_i)\\}_{i=1}^{n_{train}}$, the C-SVM formulation is given as:\n",
    "\n",
    "\\begin{align}\n",
    "\\underset{\\mathbf{w},\\boldsymbol{\\xi},b}{\\min}&\\,\\frac{1}{2}\\|\\mathbf{w}\\|^2 + C \\sum_{i=1}^{n_{train}}\\xi_i \\\\\n",
    "\\text{s.t.}&\\,y_i(\\mathbf{w}^T\\phi(\\mathbf{x}_i))+b)\\geq 1-\\xi_i\\\\\n",
    "&\\,\\xi_i\\geq 0,\\quad i=1,\\ldots,n_{train},\n",
    "\\end{align}\n",
    "\n",
    "where $\\mathbf{w}\\in\\mathbb{R}^d$ are the model parameters, and $b\\in\\mathbb{R}$ is the bias, and $\\boldsymbol{\\xi}\\in\\mathbb{R}_{\\geq0}^{n_{train}}$ is the vector of slack-variables, and $C>0\\in\\mathbb{R}$ is the regularization parameter. \n",
    "\n",
    "The primal C-SVM **decision function** is given as:\n",
    "\n",
    "$$\n",
    "f(\\mathbf{x})=\\mathbf{w}^T\\phi(\\mathbf{x})+b\\in\\mathbb{R}\n",
    "$$\n",
    "\n",
    "and the corresponding **prediction function** as: \n",
    "\n",
    "$$\n",
    "g(\\mathbf{x})=sign(f(\\mathbf{x}))\\in\\{-1,1\\}.\n",
    "$$\n",
    "\n",
    "#### SVM Dual formulation <a id='SVM_dual'></a>\n",
    "\n",
    "In the Pen & Paper exercise you showed, that the dual C-SVM can be written as:\n",
    "\n",
    "\\begin{align}\n",
    "\\underset{\\boldsymbol{\\alpha}}{\\max}&\\,\\mathcal{L}(\\boldsymbol{\\alpha})=\\underbrace{\\sum_{i=1}^{n_{train}}\\alpha_i - \\frac{1}{2}\\sum_{i=1}^{n_{train}}\\sum_{j=1}^{n_{train}}\\alpha_i\\alpha_j y_i y_j \\kappa(\\mathbf{x}_i,\\mathbf{x}_j)}_{\\text{Loss function}}\\\\\n",
    "\\text{s.t.}&\\,0\\leq\\alpha_i\\leq C,\\quad i=1,\\ldots,n_{train} \\\\\n",
    "&\\,\\underbrace{\\sum_{i=1}^{n_{train}}\\alpha_i y_i=0}_{\\text{Bias constraint}},\n",
    "\\end{align}\n",
    "\n",
    "or, equivalent in matrix notation, as:\n",
    "\n",
    "\\begin{align}\n",
    "\\underset{\\boldsymbol{\\alpha}}{\\max}&\\,\\mathbf{1}^T\\boldsymbol{\\alpha} - \\frac{1}{2}\\boldsymbol{\\alpha}^T\\left(\\mathbf{y}\\mathbf{y}^T\\circ\\mathbf{K}\\right)\\boldsymbol{\\alpha}\\\\\n",
    "\\text{s.t.}&\\,0\\leq\\alpha_i\\leq C,\\quad i=1,\\ldots,n_{train} \\\\\n",
    "&\\,\\boldsymbol{\\alpha}^T\\mathbf{y}=0,\n",
    "\\end{align}\n",
    "\n",
    "where $\\boldsymbol{\\alpha}\\in\\mathbb{R}^{n_{train}}$ are the dual variables, and $\\mathbf{K}\\in\\mathbb{R}^{n_{train}\\times n_{train}}$ is the training kernel matrix (with $[\\mathbf{K}]_{ij}=\\kappa(\\mathbf{x}_i,\\mathbf{x}_j)=\\phi(\\mathbf{x}_i)^T\\phi(\\mathbf{x}_j)$), and $\\mathbf{y}\\in\\{-1,1\\}^{n_{train}}$ training labels, and $C>0\\in\\mathbb{R}$ being the regularisation parameter. Let us furthermore define the shorthand: $\\mathbf{G}=\\mathbf{y}\\mathbf{y}^T\\circ\\mathbf{K}$.\n",
    "\n",
    "##### Support Vector (SV) <a id='SV'></a>\n",
    "\n",
    "The training examples $\\mathbf{x}_i$ (respectively their feature vectors $\\phi(\\mathbf{x}_i)$) for which $\\alpha_i>0$ are called **support vectors (SV)**. The examples $\\mathbf{x}_i$ for which *additionally* holds $\\alpha_i<C$ are the SVs **on the margin**. For convenience let us define $\\mathcal{I}_S$ being the index set of the support vectors and $\\mathcal{I}_M$ being the index set of the support vectors on the margin.\n",
    "##### Dual C-SVM **decision function**\n",
    "\n",
    "The Dual C-SVM decision function for a new exmaple $\\mathbf{x}$ can be written as:\n",
    "\n",
    "$$\n",
    "f(\\mathbf{x})=\\left(\\sum_{i=1}^{n_{train}}\\alpha_i y_i \\kappa(\\mathbf{x},\\mathbf{x}_i)\\right)+b=\\left(\\sum_{i\\in\\mathcal{I}_{S}}\\alpha_i y_i \\kappa(\\mathbf{x},\\mathbf{x}_i)\\right)+b\\in\\mathbb{R}\n",
    "$$\n",
    "\n",
    "or, equivalent in matrix notation, as:\n",
    "\n",
    "$$\n",
    "f(\\mathbf{x})=\\left(\\mathbf{y}[\\mathcal{I}_S]\\circ\\boldsymbol{\\alpha}[\\mathcal{I}_S]\\right)^T\\mathbf{k}(\\mathbf{x})[\\mathcal{I}_S] + b,\n",
    "$$\n",
    "\n",
    "with $z[\\cdot]$ element access similar to numpy, and $\\mathbf{k}(\\mathbf{x})=[\\kappa(\\mathbf{x},\\mathbf{x}_1),\\ldots,\\kappa(\\mathbf{x},\\mathbf{x}_{n_{train}})]\\in\\mathbb{R}^{n_{train}}$ are the kernel values between the new and the training examples.\n",
    "\n",
    "#### Estimating the Bias-term $b$ <a id='bias'></a>\n",
    "\n",
    "For the SVs on the margin we know that $\\xi_i=0$ and therefore that $y_i f(\\mathbf{x}_i)=1$. We can therefore calculate $b$ for a given $\\boldsymbol{\\alpha}$ using:\n",
    "\n",
    "$$\n",
    "    b = \\frac{1}{|\\mathcal{I}_M|}\\sum_{i\\in\\mathcal{I}_M}\\left(y_i - \\sum_{j\\in\\mathcal{I}_s} y_j\\alpha_j\\kappa(\\mathbf{x}_i, \\mathbf{x}_j)\\right)\n",
    "$$\n",
    "\n",
    "or, equivalent in matrix notation:\n",
    "\n",
    "$$\n",
    "    b = \\frac{1}{|\\mathcal{I}_M|}\\mathbf{1}^T\\left(\\mathbf{y}[\\mathcal{I}_M]-\\mathbf{K}[\\mathcal{I}_M][:, \\mathcal{I}_S]\\left(\\mathbf{y}[\\mathcal{I}_S]\\circ\\boldsymbol{\\alpha}[\\mathcal{I}_S]\\right)\\right).\n",
    "$$\n",
    "\n",
    "For further details, check out [\"Pattern Recognition and Machine Learning\" book by C. Bishop](https://www.microsoft.com/en-us/research/people/cmbishop/#!prml-book) (p. 333-334).\n",
    "\n",
    "### A. Dual optimization using quadratic programming (QP) (**2 Points**) <a id='task_1'></a>\n",
    "<div class=\"alert alert-success\">\n",
    "    <b>Task:</b>\n",
    "    Implement missing code parts of the SVM class. You have to modify the following member-functions:\n",
    "    \n",
    "- ```_loss_and_grad```: This function should return the loss function value $\\mathcal{L}(\\boldsymbol{\\alpha})$ for a given $\\boldsymbol{\\alpha}$ vector and the gradient $\\nabla_{\\boldsymbol{\\alpha}}\\mathcal{L}$ of the loss function.\n",
    "- ```_calculate_bias```: This function should return the bias calculated as [presented](#bias). Keep the [support vector definitions](#SV) in mind.\n",
    "- ```fit```: In this function you need to define the $\\mathbf{G}$ matrix, the box and linear constraints (*s.t.*) and run the optimizer.\n",
    "- ```decision_function``` and ```predict```: This functions should implement the functions $f(\\mathbf{x})$ and $g(\\mathbf{x})$.\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "    <b>Hints / Notes:</b> \n",
    "    \n",
    "- There are online tools existing to check your derived gradients, e.g. [matrixcalculus.org](http://www.matrixcalculus.org/).\n",
    "- Make your self familiar with the [```scipy.optimize.minimize```](https://docs.scipy.org/doc/scipy-1.1.0/reference/generated/scipy.optimize.minimize.html) function.\n",
    "- Read how to define [box- (bounds)](https://docs.scipy.org/doc/scipy-1.1.0/reference/generated/scipy.optimize.Bounds.html#scipy.optimize.Bounds) and linear-constraints?\n",
    "- The dual loss function $\\mathcal{L}$ can be maximized by minimizing $-\\mathcal{L}$. Scipy only implements a minimize function.\n",
    "- [Visualizinf your model](#visualize_svm_model) might help you debugging it.\n",
    "- Instead of the index sets $\\mathcal{I}_M$ and $\\mathcal{I}_S$, the implementation works with indicator vectors, e.g. ```_is_sv```$ =\\{True,False\\}^{n_{train}}$ with ```_is_sv[i]``` = $\\alpha_i>0$.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "svm",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class SVM(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, C=1., alpha_threshold=1e-6, sigma=None, kernel=\"gaussian\", verbose=True):\n",
    "        \"\"\"\n",
    "        C - Support Vector Machine (SVM)\n",
    "        \n",
    "        :param C: scalar, regularization parameter C (default = 1)\n",
    "        :param alpha_threshold: scalar, threshold to set the dual variables to zero if very\n",
    "            small (e.g. due to numerical optimization) (default = 1e-6)\n",
    "        :param sigma: scalar, sigma parameter used for the gaussian kernel (default = None)\n",
    "        :param kernel: {string, calaable}, name of the kernel to use or function to\n",
    "            calculate the kernel matrices (defualt = \"gaussian\")\n",
    "        :param verbose: boolean, indicating whether some performance / debugging information\n",
    "            should be plotted\n",
    "        \"\"\"\n",
    "        # Optimization parameters\n",
    "        self.C = C \n",
    "        self._alpha_threshold = alpha_threshold\n",
    "        self._verbose = verbose\n",
    "        \n",
    "        # Model parameters\n",
    "        self._alpha = None      # dual variables alpha_i\n",
    "        self._bias = None       # bias term\n",
    "        self._X_train = None    # training feature vectors needed for prediction\n",
    "        self._y_train = None     # training labels\n",
    "        \n",
    "        # Support vector information\n",
    "        self._is_sv = None            # indicator vector, which example is support vector\n",
    "        self.n_sv = None              # number of support vectors per class\n",
    "        self.support_vectors_ = None  # Support vector input feature vectors \n",
    "        \n",
    "        # Kernel parameters\n",
    "        self.kernel = kernel\n",
    "        self.sigma = sigma\n",
    "        \n",
    "    def _loss_and_grad(self, alpha, G, sign=-1):\n",
    "        \"\"\"\n",
    "        Calculate the SVM dual loss function and its corresponding gradient.\n",
    "        \n",
    "        :param alpha: array-like, shape=(n_train, ), current dual variable vector\n",
    "        :param G: array-like, shape=(n_train, n_train), G_train matrix \n",
    "        :param sign: scalar, sign of the loss and gradient, should be 1 for minimization and \n",
    "            -1 for maximization (default = -1)\n",
    "            \n",
    "        :return tuple=(loss function value, gradient vector [shape=(n_train,)])\n",
    "        \"\"\"\n",
    "        ### BEGIN SOLUTION\n",
    "        loss_value = np.sum(alpha) - 0.5 * alpha.T @ G @ alpha\n",
    "        gradient_vector = 1. - alpha.T @ G\n",
    "        ### END SOLUTION\n",
    "\n",
    "        return sign * loss_value, sign * gradient_vector\n",
    "    \n",
    "    def _calculate_bias(self, K_train, y_train, alpha, is_sv):\n",
    "        \"\"\"\n",
    "        Function to determine the bias term after the dual variables have been optimized.\n",
    "        \n",
    "        :param K_train: array-like, shape=(n_train, n_train), training kernel matrix\n",
    "        :param y_train: array-like, shape=(n_train, 1), training labels\n",
    "        :param alpha: array-like, shape=(n_train,), dual variables\n",
    "        :param is_sv: array-like, shape=(n_train,), boolean vector indicating whether a \n",
    "            training example is a support vector or not, i.e. is_cv[i] == True => alpha[i] > 0 \n",
    "        \n",
    "        :return: scalar, bias\n",
    "        \"\"\"\n",
    "        # Get indicator vector of the support vectors on the margin, \n",
    "        # i.e. for which slack_i = 0.\n",
    "        is_sv_mrg = np.bitwise_and(is_sv, (alpha < self.C).flatten())\n",
    "        \n",
    "        # Calculate the bias according to the formula. \n",
    "        ### BEGIN SOLUTION\n",
    "        bias = np.mean(y_train[is_sv_mrg] - \n",
    "                       (K_train[is_sv_mrg][:, is_sv] @ (y_train[is_sv] * alpha[is_sv])))\n",
    "        ### END SOLUTION\n",
    "         \n",
    "        return bias\n",
    "        \n",
    "    def fit(self, X_train, y_train):\n",
    "        \"\"\"\n",
    "        Fit the SVM model parameters\n",
    "        \n",
    "        :param X_train: array-like, shape=(n_train, n_features), training feature matrix\n",
    "        :param y_train: array-like, shape=(ntrain, ) or (n_train, 1), training labels, {-1, 1}\n",
    "        \n",
    "        :return: reference to it self\n",
    "        \"\"\"\n",
    "        self._X_train = X_train\n",
    "        K_train = self._get_kernel(self._X_train)\n",
    "        n_train = K_train.shape[0]  # number of training examples\n",
    "        \n",
    "        # Make training labels beeing a column-vector\n",
    "        self._y_train = y_train\n",
    "        if len(self._y_train.shape) == 1:\n",
    "            self._y_train = self._y_train[:, np.newaxis]\n",
    "        \n",
    "        # Calculate the matrix: G_train = yy' .* K_train\n",
    "        ### BEGIN SOLUTION\n",
    "        G_train = np.outer(self._y_train, self._y_train) * K_train\n",
    "        ### END SOLUTION\n",
    "        \n",
    "        # Set up the equality contraint introduced by the bias-term: bias_const\n",
    "        ### BEGIN SOLUTION\n",
    "        bias_const = {\"type\": \"eq\", \"fun\": lambda alpha: self._y_train.T @ alpha}\n",
    "        ### END SOLUTION\n",
    "        \n",
    "        assert (isinstance(bias_const, dict) and \\\n",
    "                \"type\" in bias_const and \\\n",
    "                \"fun\" in bias_const), \\\n",
    "            \"bias_const must be specified as dictionary. See hints.\"\n",
    "        assert (callable(bias_const[\"fun\"])), \"Provide a function to evalute the constraint.\"\n",
    "        \n",
    "        # Define the bounds (0 <= alpha_i <= C) for the dual varaibles: bound_const\n",
    "        ### BEGIN SOLUTION\n",
    "        bounds_const = spopt.Bounds(0, self.C)\n",
    "        ### END SOLUTION\n",
    "        \n",
    "        assert (isinstance(bounds_const, spopt.Bounds))\n",
    "        \n",
    "        # Define a feasiable initial value for the dual variables: \n",
    "        # 0 <= alpha0_i <= C, y^T alpha0_i = 0\n",
    "        ### BEGIN SOLUTION\n",
    "        alpha0 = np.zeros((n_train, ))\n",
    "        ### END SOLUTION\n",
    "        \n",
    "        assert (alpha0.shape == (n_train, )), \"alpha0 must have shape=(n_train,).\"\n",
    "        assert (all(alpha0 >= 0) and all(alpha0 <= self.C) and (self._y_train.T @ alpha0 == 0)), \\\n",
    "            \"alpha0 must be feasible.\"\n",
    "        \n",
    "        if self._verbose:\n",
    "            start = time.time()\n",
    "        \n",
    "        # Run the optimizer\n",
    "        res = spopt.minimize(self._loss_and_grad, x0=alpha0, jac=True, args=(G_train, ),\n",
    "                             method=\"SLSQP\", constraints=bias_const, bounds=bounds_const)\n",
    "        \n",
    "        if self._verbose:\n",
    "            print(\"Optimizing time: %.3fs\" % (time.time() - start))\n",
    "        \n",
    "        # Extract the optimal dual varaibles (solution) from the optimizer\n",
    "        self._alpha = res[\"x\"][:, np.newaxis]\n",
    "        \n",
    "        # Threshold alpha values to zero if very small\n",
    "        self._alpha[self._alpha < self._alpha_threshold] = 0\n",
    "        self._alpha[np.abs(self._alpha - self.C) < self._alpha_threshold] = self.C\n",
    "        \n",
    "        # Find support vectors (alpha_i > 0)\n",
    "        ### BEGIN SOLUTION\n",
    "        self._is_sv = (self._alpha > 0).flatten()\n",
    "        ### END SOLUTION\n",
    "        \n",
    "        assert (self._is_sv.shape == (n_train, )), \\\n",
    "            \"_is_sv must be an indicator vector with shape=(n_train, ).\"\n",
    "        \n",
    "        self.support_vectors_ = X_train[self._is_sv]\n",
    "        \n",
    "        # Get number of support vectors per class\n",
    "        self.n_sv = np.array([np.sum(self._is_sv[y_train.flatten() == -1]),\n",
    "                              np.sum(self._is_sv[y_train.flatten() ==  1])])\n",
    "        \n",
    "        # Calcualte the bias\n",
    "        self._bias = self._calculate_bias(K_train, self._y_train, self._alpha, self._is_sv)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def decision_function(self, X):\n",
    "        \"\"\"\n",
    "        Calculate decision function:\n",
    "            f(x) = sum_i y_i alpha_i k(x_i, x) + bias\n",
    "\n",
    "        :param X: array-like, shape=(n_test, n_features), \n",
    "        :return: array-like, shape=(n_test, ), decision function value f(x) for all test \n",
    "            samples\n",
    "        \"\"\"\n",
    "        # Calculate the test-training kernel shape=(n_test, n_train)\n",
    "        K_test_train = self._get_kernel(X, self._X_train)\n",
    "        \n",
    "        # Calculate the decision function values (only using SV)\n",
    "        ### BEGIN SOLUTION\n",
    "        g_X = K_test_train[:, self._is_sv] @ (\n",
    "            self._alpha[self._is_sv] * self._y_train[self._is_sv]) + self._bias\n",
    "        ### END SOLUTION\n",
    "        \n",
    "        # reduce to 1d vector\n",
    "        g_X = g_X.flatten()\n",
    "        \n",
    "        # check output dimension\n",
    "        assert (g_X.shape == (X.shape[0], )), \\\n",
    "            \"Output of the decision function must have shape: (n_test, )\"\n",
    "        \n",
    "        return g_X\n",
    "        \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict labels using C-SVM:\n",
    "            g(x) = sign(f(x)), with f(x) being the decision function\n",
    "\n",
    "        :param X: array-like, shape=(n_test, n_features), test feature matrix\n",
    "        :return: array-like, shape=(n_test,), predicted labels {-1, 1} for all test samples\n",
    "        \"\"\"\n",
    "        ### BEGIN SOLUTION\n",
    "        y_X = np.sign(self.decision_function(X))\n",
    "        ### END SOLUTION\n",
    "        \n",
    "        assert ((np.in1d(y_X, [-1, 0, 1]).all())), \\\n",
    "            \"Output of the prediction function must be {-1, 0, 1}\"\n",
    "        \n",
    "        return y_X\n",
    "    \n",
    "    def _get_kernel(self, X, Y=None):\n",
    "        \"\"\"\n",
    "        Caculate kernel matrix using specified kernel-function and parameters.\n",
    "\n",
    "        :param X: array-like, shape=(n_samples_A, n_features), feature-matrix of set A\n",
    "        :param Y: array-like, shape=(n_samples_B, n_features), feature-matrix of set B\n",
    "            or None, than Y = X\n",
    "        :return: array-like, shape=(n_samples_A, n_samples_B), kernel matrix\n",
    "        \"\"\"\n",
    "        if self.kernel == \"gaussian\":\n",
    "            return gaussian_kernel_wrapper(X, Y, self.sigma)\n",
    "        elif self.kernel == \"linear\":\n",
    "            return linear_kernel_sk(X, Y)\n",
    "        elif callable(self.kernel):\n",
    "            return self.kernel(X, Y)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid kernel chosen.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tests for the ```_loss_and_grad``` Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "svm_loss_test",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Tests for the loss & gradient function \n",
    "\n",
    "# Very simple data\n",
    "# __X = np.array([[0, 1], [-1, 0], [0, -1], [1, 0]]) \n",
    "# __y = np.array([1, 1, -1, -1])\n",
    "\n",
    "# Linear kernel\n",
    "__G = np.array([[1, 0, 1, 0], [0, 1, 0, 1], [1, 0, 1, 0], [0, 1, 0, 1]])  # assume linear K\n",
    "__alpha = np.full((4, ), 0.5)\n",
    "__loss_val, __grad_vec = SVM(C=1.)._loss_and_grad(__alpha, __G)\n",
    "\n",
    "assert (np.isscalar(__loss_val)), \"Loss value must be a scalar.\"\n",
    "assert (__grad_vec.shape == __alpha.shape), \"Gradient vector must have same length as alpha.\"\n",
    "\n",
    "np.testing.assert_allclose(__loss_val, - (2. - 0.5 * 2.),\n",
    "                           err_msg=\"Loss value is wrong.\")  # remember the negative sign of loss\n",
    "np.testing.assert_allclose(__grad_vec, np.zeros((4, )), \n",
    "                           err_msg=\"Gradient vector is wrong.\")\n",
    "\n",
    "# Gaussian kernel\n",
    "__G = np.array([[ 1.   ,  0.368,  0.135,  0.368],\n",
    "                [ 0.368,  1.   ,  0.368,  0.135],\n",
    "                [ 0.135,  0.368,  1.   ,  0.368],\n",
    "                [ 0.368,  0.135,  0.368,  1.   ]])  # assume gaussian K\n",
    "__alpha = np.ones((4, ))\n",
    "__loss_val, __grad_vec = SVM(C=1., kernel=\"gaussian\")._loss_and_grad(__alpha, __G)\n",
    "\n",
    "np.testing.assert_allclose(__loss_val, - (4. - 0.5 * 7.484),\n",
    "                           err_msg=\"Loss value is wrong.\")  # remember the negative sign of loss\n",
    "np.testing.assert_allclose(__grad_vec, np.full((4, ), 0.871),\n",
    "                           err_msg=\"Gradient vector is wrong.\")\n",
    "\n",
    "### BEGIN HIDDEN TESTS\n",
    "def ____loss_and_grad_REF(mu, G, sign=-1):\n",
    "    \"\"\"\n",
    "    Calculate the SVM dual loss function and its corresponding gradient.\n",
    "    \"\"\"\n",
    "    loss_value = np.ones((1, mu.shape[0])) @ mu - mu.T @ G @ mu / 2.\n",
    "    gradient_vector = 1. - mu.T @ G\n",
    "    \n",
    "    return sign * loss_value, sign * gradient_vector\n",
    "\n",
    "____X = np.random.RandomState(728).rand(31, 23)\n",
    "____y = np.sign(np.random.RandomState(728).randn(31, 1))\n",
    "____G = ____y @ ____y.T * gaussian_kernel_wrapper(____X)\n",
    "____alpha = np.random.RandomState(201).rand(31)\n",
    "\n",
    "____loss_val, ____grad_vec = SVM(C=1.)._loss_and_grad(____alpha, ____G)\n",
    "____loss_val_REF, ____grad_vec_REF = ____loss_and_grad_REF(____alpha, ____G)\n",
    "\n",
    "np.testing.assert_allclose(____loss_val, ____loss_val_REF)\n",
    "np.testing.assert_allclose(____grad_vec, ____grad_vec_REF)\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tests for the ```_calculate_bias``` Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "svm_bias_test",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Test for the bias calculation\n",
    "\n",
    "# Simple data (linear separable)\n",
    "__X = np.array([[-1, 0], [0, 1], [-.75, .75], [0, -1], [1, 0]])\n",
    "__y = np.array([1, 1, 1, -1, -1])\n",
    "__C = 10.\n",
    "\n",
    "# Get dual variable vector using the sklearn SVM\n",
    "__svm_sk = SVM_sk(C=__C, kernel=\"linear\").fit(__X, __y)\n",
    "__alpha = np.zeros((__X.shape[0], ))\n",
    "# Note: Sklearn stores only dual_coef_i = alpha_i * y_i\n",
    "__alpha[__svm_sk.support_] = __svm_sk.dual_coef_ * __y[__svm_sk.support_]  \n",
    "\n",
    "# Determine support vectors\n",
    "__is_sv = (__alpha > 0)\n",
    "\n",
    "# Calculate the bias\n",
    "__bias = SVM(C=__C, kernel=\"linear\")._calculate_bias(linear_kernel_sk(__X), __y, \n",
    "                                                     __alpha, __is_sv)\n",
    "np.testing.assert_array_equal(__bias, __svm_sk.intercept_, err_msg=\"Bias term is not correct.\")\n",
    "\n",
    "# Simple data (linear separable): shifted data\n",
    "# Get dual variable vector using the sklearn SVM\n",
    "__X += np.array([-1., 1.])\n",
    "__svm_sk = SVM_sk(C=__C, kernel=\"linear\").fit(__X, __y)\n",
    "__alpha = np.zeros((__X.shape[0], ))\n",
    "__alpha[__svm_sk.support_] = __svm_sk.dual_coef_ * __y[__svm_sk.support_]\n",
    "\n",
    "# Determine support vectors\n",
    "__is_sv = (__alpha > 0)\n",
    "\n",
    "# Calculate the bias\n",
    "__bias = SVM(C=__C, kernel=\"linear\")._calculate_bias(linear_kernel_sk(__X), \n",
    "                                                     __y, __alpha, __is_sv)\n",
    "np.testing.assert_array_equal(__bias, __svm_sk.intercept_, err_msg=\"Bias term is not correct.\")\n",
    "\n",
    "# Simple data (not separable)\n",
    "__X = np.array([[-1, 0], [0, 1], [.75, -.75], [0, -1], [1, 0]]) + np.array([-1., 1.])\n",
    "__y = np.array([1, 1, 1, -1, -1])\n",
    "__C = 10.\n",
    "\n",
    "# Get dual variable vector using the sklearn SVM\n",
    "__svm_sk = SVM_sk(C=__C, kernel=\"linear\").fit(__X, __y)\n",
    "__alpha = np.zeros((__X.shape[0], ))\n",
    "__alpha[__svm_sk.support_] = __svm_sk.dual_coef_ * __y[__svm_sk.support_]\n",
    "\n",
    "# Determine support vectors\n",
    "__is_sv = (__alpha > 0)\n",
    "\n",
    "# Calculate the bias\n",
    "__bias = SVM(C=__C, kernel=\"linear\")._calculate_bias(linear_kernel_sk(__X), __y,\n",
    "                                                     __alpha, __is_sv)\n",
    "np.testing.assert_allclose(__bias, __svm_sk.intercept_, \n",
    "                           err_msg=\"Bias term is not correct.\", atol=1e-6)\n",
    "\n",
    "### BEGIN HIDDEN TESTS\n",
    "____X, ____y = make_moons(n_samples=150, noise=0.25, random_state=921)\n",
    "____y[____y == 0] = -1\n",
    "____C = 2.\n",
    "\n",
    "# Get dual variable vector using the sklearn SVM\n",
    "____svm_sk = SVM_sk(C=____C, kernel=\"rbf\", gamma=\"auto\").fit(____X, ____y)\n",
    "____alpha = np.zeros((____X.shape[0], ))\n",
    "____alpha[____svm_sk.support_] = ____svm_sk.dual_coef_ * ____y[____svm_sk.support_]\n",
    "\n",
    "# Determine support vectors\n",
    "____is_sv = (____alpha > 0)\n",
    "\n",
    "# Calculate the bias\n",
    "____bias = SVM(C=____C, kernel=\"gaussian\")._calculate_bias(\n",
    "    gaussian_kernel_wrapper(____X), ____y, ____alpha, ____is_sv)\n",
    "np.testing.assert_allclose(____bias, ____svm_sk.intercept_, \n",
    "                           err_msg=\"Bias term is not correct.\", atol=1e-6)\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visual Inspection of your C-SVM Implementation <a id='visualize_svm_model'></a>\n",
    "\n",
    "Here we run a small classification problem, that is linearly seperable. The example is taken from the sklearn-package [Maximum margin separating hyperplane example](https://scikit-learn.org/stable/auto_examples/svm/plot_separating_hyperplane.html). Your model should have *three* support vectors (one red -, two blue +). Your estimated bias should $b=-3.2145$ and your dual variables (of the support vectors, i.e. $\\alpha_i > 0$) $\\boldsymbol{\\alpha}=[0.3834, 0.2537, 0.1297]^T$. \n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "    <b>Note:</b>  Here for first time we actually run to optimizer. So if anything goes wrong, check that, e.g., your constraints are set up correctly and your prediction functions are outputting the right format.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create some very simple test data\n",
    "X, y = make_blobs(n_samples=40, centers=2, random_state=6)\n",
    "y[y==0] = -1\n",
    "plot_svm_model(X, y, SVM(C=1, kernel=\"linear\", verbose=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Comparison to Sklearn SVM (libSVM) (1 Point) <a id='task_2'></a>\n",
    "\n",
    "In this task your SVM implementation will be applied on two artificial datasets (see also exericse 1) and compared with the performance of the [sklearn SVC](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC) implementation. Sklearn uses [libSVM](https://www.csie.ntu.edu.tw/~cjlin/libsvm/) as solver in the background. If you are interested in SVM solver implementation details, you can read the [libSVM manual](https://www.csie.ntu.edu.tw/~cjlin/papers/libsvm.pdf). \n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "    <b>Task:</b> Your SVM implentation is applied here, i.e. do <b>not need</b> to write additional code. Simply run tests in the next cell. It will test all the functionality of your SVM class. Revise your implementation in case some tests fail.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "svm_vs_sklearn_test",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Test implementation agains sklearn\n",
    "__X_blobs, __y_blobs = make_blobs(n_samples=350, centers=[[1, 1], [3, 3]],\n",
    "                                  cluster_std=[0.5, 1.15], random_state=202)\n",
    "__X_moons, __y_moons = make_moons(n_samples=350, noise=0.25, random_state=212)\n",
    "\n",
    "# Make labels being {-1, 1}\n",
    "__y_blobs[__y_blobs==0] = -1\n",
    "__y_moons[__y_moons==0] = -1\n",
    "\n",
    "# Split data\n",
    "__X_blobs_train, __X_blobs_test, __y_blobs_train, __y_blobs_test = train_test_split(\n",
    "    __X_blobs, __y_blobs, random_state=191)\n",
    "__X_moons_train, __X_moons_test, __y_moons_train, __y_moons_test = train_test_split(\n",
    "    __X_moons, __y_moons, random_state=881)\n",
    "\n",
    "# Blobs\n",
    "print(\"Blobs:\", end=\"\\n\\t\")\n",
    "__svm_sk = SVM_sk(C=2., kernel=\"rbf\", gamma=\"auto\").fit(__X_blobs_train, __y_blobs_train)\n",
    "__svm = SVM(C=2., kernel=\"gaussian\").fit(__X_blobs_train, __y_blobs_train)\n",
    "\n",
    "print(\"\\tTest score (sklearn, scipy):\", \n",
    "      np.round(__svm_sk.score(__X_blobs_test, __y_blobs_test), 3), \n",
    "      np.round(__svm.score(__X_blobs_test, __y_blobs_test), 3))\n",
    "print(\"\\tN_sv (sklearn, scipy):\", __svm_sk.n_support_, __svm.n_sv)\n",
    "print(\"\\tBias (sklearn, scipy):\", np.round(__svm_sk.intercept_, 3), np.round(__svm._bias, 3))\n",
    "\n",
    "np.testing.assert_allclose(__svm.score(__X_blobs_test, __y_blobs_test), \n",
    "                           __svm_sk.score(__X_blobs_test, __y_blobs_test), \n",
    "                           err_msg=\"Blobs: Test set accuracies differ too much.\")\n",
    "np.testing.assert_equal(__svm.n_sv, __svm_sk.n_support_,\n",
    "                        err_msg=\"Moons: Number of support vectors does not match.\")\n",
    "np.testing.assert_allclose(__svm._bias, __svm_sk.intercept_, atol=1e-2, \n",
    "                           err_msg=\"Blobs: Bias values differ too much.\")\n",
    "\n",
    "# Moons\n",
    "print(\"Moons:\", end=\"\\n\\t\")\n",
    "__svm_sk = SVM_sk(C=2., kernel=\"rbf\", gamma=\"auto\").fit(__X_moons_train, __y_moons_train)\n",
    "__svm = SVM(C=2., kernel=\"gaussian\").fit(__X_moons_train, __y_moons_train)\n",
    "\n",
    "print(\"\\tTest score (sklearn, scipy):\", \n",
    "      np.round(__svm_sk.score(__X_moons_test, __y_moons_test), 3), \n",
    "      np.round(__svm.score(__X_moons_test, __y_moons_test), 3))\n",
    "print(\"\\tN_sv (sklearn, scipy):\", __svm_sk.n_support_, __svm.n_sv)\n",
    "print(\"\\tBias (sklearn, scipy):\", np.round(__svm_sk.intercept_, 3), np.round(__svm._bias, 3))\n",
    "\n",
    "np.testing.assert_allclose(__svm.score(__X_moons_test, __y_moons_test), \n",
    "                           __svm_sk.score(__X_moons_test, __y_moons_test), \n",
    "                           err_msg=\"Moons: Test set accuracies differ too much.\")\n",
    "np.testing.assert_equal(__svm.n_sv, __svm_sk.n_support_, \n",
    "                        err_msg=\"Moons: Number of support vectors does not match.\")\n",
    "np.testing.assert_allclose(__svm._bias, __svm_sk.intercept_, atol=1e-2, \n",
    "                           err_msg=\"Moons: Bias values differ too much.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. Visualization of the Model and Support Vectors <a id='visual_sv'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create synthetic data (Please do not change the random_state!)\n",
    "X_blobs, y_blobs = make_blobs(n_samples=250, centers=[[1, 1], [3, 3]], cluster_std=[0.5, 1.15], random_state=202)\n",
    "X_moons, y_moons = make_moons(n_samples=250, noise=0.25, random_state=211)\n",
    "# Make labels being {-1, 1}\n",
    "y_blobs[y_blobs==0] = -1\n",
    "y_moons[y_moons==0] = -1\n",
    "\n",
    "# Split data\n",
    "X_blobs_train, X_blobs_test, y_blobs_train, y_blobs_test = train_test_split(X_blobs, y_blobs, random_state=191)\n",
    "X_moons_train, X_moons_test, y_moons_train, y_moons_test = train_test_split(X_moons, y_moons, random_state=881)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot datasets\n",
    "fig, axrr = plt.subplots(1, 2, figsize=(20, 7))\n",
    "\n",
    "# Blobs\n",
    "for l_str, l_num, col in [(\"negative\", -1, \"red\"), (\"postive\", 1, \"blue\")]:    \n",
    "    axrr[0].scatter(\n",
    "        X_blobs_train[y_blobs_train==l_num, 0], X_blobs_train[y_blobs_train==l_num, 1],\n",
    "        c=col, alpha=0.65, label=\"Train (%s)\" % l_str)\n",
    "    \n",
    "    axrr[0].scatter(\n",
    "        X_blobs_test[y_blobs_test==l_num, 0], X_blobs_test[y_blobs_test==l_num, 1],\n",
    "        c=col, alpha=0.65, label=\"Test (%s)\" % l_str, marker=\"x\")\n",
    "        \n",
    "# Blobs\n",
    "for l_str, l_num, col in [(\"negative\", -1, \"red\"), (\"postive\", 1, \"blue\")]:    \n",
    "    axrr[1].scatter(\n",
    "        X_moons_train[y_moons_train==l_num, 0], X_moons_train[y_moons_train==l_num, 1],\n",
    "        c=col, alpha=0.65, label=\"Train (%s)\" % l_str)\n",
    "    \n",
    "    axrr[1].scatter(\n",
    "        X_moons_test[y_moons_test==l_num, 0], X_moons_test[y_moons_test==l_num, 1],\n",
    "        c=col, alpha=0.65, label=\"Test (%s)\" % l_str, marker=\"x\")\n",
    "\n",
    "\n",
    "axrr[0].set_title(\"Random blobs\", fontsize=\"xx-large\")\n",
    "axrr[0].legend(title=\"Data points\", fontsize=\"x-large\", scatterpoints=3, edgecolor=\"k\")\n",
    "\n",
    "axrr[1].set_title(\"Moons\", fontsize=\"xx-large\")\n",
    "axrr[1].legend(title=\"Data points\", fontsize=\"x-large\", scatterpoints=3, edgecolor=\"k\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axrr = plt.subplots(2, 2, figsize=(20, 20))\n",
    "plot_svm_model(X_blobs_train, y_blobs_train, SVM(C=10., kernel=\"linear\"), \n",
    "               ax=axrr[0, 0], verbose=False)\n",
    "axrr[0, 0].set_title(\"Blobs (linear)\")\n",
    "plot_svm_model(X_blobs_train, y_blobs_train, SVM(C=10., kernel=\"gaussian\", sigma=1.5),\n",
    "               ax=axrr[0, 1], verbose=False)\n",
    "axrr[0, 1].set_title(\"Blobs (gaussian)\")\n",
    "plot_svm_model(X_moons_train, y_moons_train, SVM(C=10., kernel=\"linear\"), \n",
    "               ax=axrr[1, 0], verbose=False)\n",
    "axrr[1, 0].set_title(\"Moons (linear)\")\n",
    "plot_svm_model(X_moons_train, y_moons_train, SVM(C=10., kernel=\"gaussian\", sigma=1.5),\n",
    "               ax=axrr[1, 1], verbose=False)\n",
    "_ = axrr[1, 1].set_title(\"Moons (gaussian)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Non-linear Kernels <a id='non_linear_kernels'></a>\n",
    "\n",
    "In this task you are going to implement two non-linear hyper-parameter free kernels for binary and non-negative feature vectors. Assume, we are given two sets of feature vectors $\\mathbf{X}_A\\in\\mathbb{R}^{n_A\\times d}, \\mathbf{X}_B\\in\\mathbb{R}^{n_B\\times d}$, where $d$ is the dimension of the feature vectors, and $n_a$ and $n_B$ are the number of examples in set $A$ respectively $B$.\n",
    "\n",
    "### A. Tanimoto-Kernel for Binary Data (1 Point) <a id='tanimoto_kernel'></a>\n",
    "\n",
    "The tanimoto kernel is used to calculate the similarities for binary input data. It calculates the normalized intersection between two sets and is also known as [Jaccard Index](https://en.wikipedia.org/wiki/Jaccard_index). \n",
    "\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "    <b>Task:</b>\n",
    "    Implement missing code parts of the function calculation the Tanimoto kernel matrix given two feature vector matrices $\\mathbf{X}_A$ and $\\mathbf{X}_B$. The resulting kernel matrix $\\mathbf{K}_{tan}$ must have dimension $n_A\\times n_B$. For a single entry in the kernel matrix it must hold:\n",
    "    \n",
    "$$\n",
    "[\\mathbf{K}_{tan}]_{ij}=\\kappa_{tan}(\\mathbf{x}_i,\\mathbf{x}_j) = \\frac{\\mathbf{x}_i^T\\mathbf{x}_j}{\\mathbf{x}_i^T\\mathbf{x}_i + \\mathbf{x}_j^T\\mathbf{x}_j - \\mathbf{x}_i^T\\mathbf{x}_j},\n",
    "$$\n",
    "\n",
    "where $\\mathbf{x}_i, \\mathbf{x}_j \\in \\{0,1\\}^d$ are two binary vectors from set $A$ respectively $B$.\n",
    "</div>\n",
    "\n",
    "Note that, the kernel values are normalized, i.e. $\\kappa_{tan}(\\mathbf{x}_i,\\mathbf{x}_j)\\in[0, 1]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "tanimoto_kernel",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def tanimoto_kernel(X, Y=None):\n",
    "    \"\"\"\n",
    "    Tanimoto kernel function\n",
    "    \n",
    "    :param X: array-like, shape=(n_samples_A, n_features), feature matrix of set A\n",
    "    :param Y: array-like, shape=(n_samples_B, n_features), feature matrix of set B \n",
    "        or None, than Y = X\n",
    "        \n",
    "    :return array-like, shape=(n_samples_A, n_samples_B), tanimoto kernel matrix\n",
    "    \"\"\"\n",
    "    if Y is None:\n",
    "        Y = X\n",
    "    \n",
    "    ### BEGIN SOLUTION\n",
    "    XY = X @ Y.T\n",
    "    XX = np.sum(X * X, axis=1)[:, np.newaxis]\n",
    "    YY = np.sum(Y * Y, axis=1)[:, np.newaxis]\n",
    "    K = XY / (XX + YY.T - XY)\n",
    "    ### END SOLUTION\n",
    "\n",
    "    return K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "tanimoto_kernel_test",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "__X_A = np.array([[1, 1, 0], [0, 1, 1], [1, 0, 0]])\n",
    "__X_B = np.array([[1, 0, 1], [1, 1, 1], [0, 0, 0], [1, 1, 0]])\n",
    "\n",
    "# symmetric kernel\n",
    "__K = tanimoto_kernel(__X_A)\n",
    "np.testing.assert_equal(__K.shape, (3, 3))\n",
    "np.testing.assert_equal(np.diag(__K), np.ones((3, )))\n",
    "np.testing.assert_equal(__K[0, 1], 1. / 3.)\n",
    "np.testing.assert_equal(__K[1, 0], 1. / 3.)\n",
    "np.testing.assert_equal(__K[0, 2], 1. / 2.)\n",
    "np.testing.assert_equal(__K[2, 0], 1. / 2.)\n",
    "assert(np.max(__K) <= 1.), \"Kernel values must be <= 1\"\n",
    "assert(np.min(__K) >= 0.), \"Kernel values must be >= 0\"\n",
    "\n",
    "# non-symmetric kernel\n",
    "__K = tanimoto_kernel(__X_A, __X_B)\n",
    "np.testing.assert_equal(__K.shape, (3, 4))\n",
    "np.testing.assert_equal(__K[0, 1], 2. / 3.)\n",
    "np.testing.assert_equal(__K[1, 0], 1. / 3.)\n",
    "np.testing.assert_equal(__K[0, 2], 0.)\n",
    "np.testing.assert_equal(__K[2, 0], 1. / 2.)\n",
    "assert(np.max(__K) <= 1.), \"Kernel values must be <= 1\"\n",
    "assert(np.min(__K) >= 0.), \"Kernel values must be >= 0\"\n",
    "\n",
    "### BEGIN HIDDEN TESTS\n",
    "def ___jacscr(x, y):\n",
    "    \"\"\"\n",
    "    Calculate Jaccard score\n",
    "    \"\"\"\n",
    "    x, y = set(np.where(x)[0]), set(np.where(y)[0])\n",
    "    \n",
    "    n_inter = len(x & y)\n",
    "    n_union = len(x | y)\n",
    "    \n",
    "    return n_inter / n_union\n",
    "    \n",
    "___X_A = np.random.RandomState(493).randint(0, 2, size=(51, 32))\n",
    "___X_B = np.random.RandomState(493).randint(0, 2, size=(12, 32))\n",
    "\n",
    "# symmetric kernel\n",
    "___K = tanimoto_kernel(___X_A)\n",
    "np.testing.assert_equal(___K.shape, (51, 51))\n",
    "np.testing.assert_equal(___K[3, 6], ___jacscr(___X_A[3], ___X_A[6]))\n",
    "np.testing.assert_equal(___K[1, 1], ___jacscr(___X_A[1], ___X_A[1]))\n",
    "np.testing.assert_equal(___K[0, 9], ___jacscr(___X_A[0], ___X_A[9]))\n",
    "np.testing.assert_equal(___K[5, 10], ___jacscr(___X_A[5], ___X_A[10]))\n",
    "np.testing.assert_equal(___K[6, 3], ___K[3, 6])\n",
    "np.testing.assert_equal(___K[0, 9], ___K[9, 0])\n",
    "np.testing.assert_equal(___K[5, 10], ___K[10, 5])\n",
    "np.testing.assert_equal(np.diag(___K), np.ones((51, )))\n",
    "\n",
    "# non-symmetric kernel\n",
    "___K = tanimoto_kernel(___X_A, ___X_B)\n",
    "np.testing.assert_equal(___K.shape, (51, 12))\n",
    "np.testing.assert_equal(___K[3, 6], ___jacscr(___X_A[3], ___X_B[6]))\n",
    "np.testing.assert_equal(___K[1, 1], ___jacscr(___X_A[1], ___X_B[1]))\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. MinMax-Kernel for Non-negative Data (1 Point) <a id='minmax_kernel'></a>\n",
    "\n",
    "The MinMax-Kernel is a normalized formulation of the intersection kernel for non-negative data, e.g. popular in image-processing and for counting data.\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "    <b>Task:</b>\n",
    "Implement missing code parts of the function calculation the MinMax kernel matrix given two feature vector matrices $\\mathbf{X}_A$ and $\\mathbf{X}_B$. The resulting kernel matrix $\\mathbf{K}_{minmax}$ must have dimension $n_A\\times n_B$. For a single entry in the kernel matrix it must hold:\n",
    "\n",
    "$$\n",
    "    [\\mathbf{K}_{minmax}]_{ij}=\\kappa_{minmax}(\\mathbf{x}_i, \\mathbf{x}_j) = \\frac{\\sum_{s=1}^d\\min(\\mathbf{x}_i^{(s)}, \\mathbf{x}_j^{(s)})}{\\sum_{s=1}^d\\max(\\mathbf{x}_i^{(s)}, \\mathbf{x}_j^{(s)})},\n",
    "$$\n",
    "\n",
    "where $\\mathbf{x}_i, \\mathbf{x}_j \\in \\mathbb{N}_0^d$ are two non-negative feature vectors.\n",
    "</div>\n",
    "\n",
    "Note, the kernel values are normalized, i.e. $\\kappa_{minmax}(\\mathbf{x}_i,\\mathbf{x}_j)\\in[0, 1]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "minmax_kernel",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def minmax_kernel(X, Y=None):\n",
    "    \"\"\"\n",
    "    Min-Max kernel function\n",
    "    \n",
    "    :param X: array-like, shape=(n_samples_A, n_features), feature matrix of set A\n",
    "    :param Y: array-like, shape=(n_samples_B, n_features), feature matrix of set B \n",
    "        or None, than Y = X\n",
    "        \n",
    "    :return array-like, shape=(n_samples_A, n_samples_B), minmax kernel matrix\n",
    "    \"\"\"\n",
    "    if Y is None:\n",
    "        Y = X\n",
    "        \n",
    "    n_A, n_B = X.shape[0], Y.shape[0]\n",
    "    \n",
    "    ### BEGIN SOLUTION\n",
    "    min_K = np.zeros((n_A, n_B))\n",
    "    max_K = np.zeros((n_A, n_B))\n",
    "    \n",
    "    for s in range(X.shape[1]):\n",
    "        c_s_A = X[:, s][:, np.newaxis]\n",
    "        c_s_B = Y[:, s][:, np.newaxis]\n",
    "        \n",
    "        min_K += np.minimum(c_s_A, c_s_B.T)\n",
    "        max_K += np.maximum(c_s_A, c_s_B.T)\n",
    "        \n",
    "    K = min_K / max_K\n",
    "    ### END SOLUTION\n",
    "        \n",
    "    return K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "minmax_kernel_test",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Test on some small data\n",
    "__X_A = np.array([[0, 1, 2], [1, 0, 0], [3, 4, 0]])\n",
    "__X_B = np.array([[0, 0, 1], [3, 1, 0]])\n",
    "\n",
    "__K = minmax_kernel(__X_A)\n",
    "np.testing.assert_array_equal(np.diag(__K), np.ones((3,)))\n",
    "np.testing.assert_equal(__K.shape, (3, 3))\n",
    "assert(np.max(__K) <= 1.), \"Kernel values must be <= 1\"\n",
    "assert(np.min(__K) >= 0.), \"Kernel values must be >= 0\"\n",
    "np.testing.assert_equal(__K[0, 1], 0.)\n",
    "np.testing.assert_equal(__K[1, 0], 0.)\n",
    "np.testing.assert_equal(__K[0, 2], 1. / 9.)\n",
    "np.testing.assert_equal(__K[2, 0], 1. / 9.)\n",
    "np.testing.assert_equal(__K[1, 2], 1. / 7.)\n",
    "np.testing.assert_equal(__K[2, 1], 1. / 7.)\n",
    "\n",
    "__K = minmax_kernel(__X_A, __X_B)\n",
    "np.testing.assert_equal(__K.shape, (3, 2))\n",
    "assert(np.max(__K) <= 1.), \"Kernel values must be <= 1\"\n",
    "assert(np.min(__K) >= 0.), \"Kernel values must be >= 0\"\n",
    "np.testing.assert_equal(__K[0, 1], 1. / 6.)\n",
    "np.testing.assert_equal(__K[1, 0], 0.)\n",
    "np.testing.assert_equal(__K[1, 1], 1. / 4.)\n",
    "np.testing.assert_equal(__K[2, 1], 4. / 7.)\n",
    "\n",
    "### BEGIN HIDDEN TESTS\n",
    "import itertools as ___it\n",
    "def ___minmax_kernel_slow(X, Y=None):\n",
    "    if Y is None:\n",
    "        Y = X\n",
    "\n",
    "    n_A, n_B = X.shape[0], Y.shape[0]\n",
    "        \n",
    "    K = np.zeros((n_A, n_B))\n",
    "        \n",
    "    for (i, j) in ___it.product(range(n_A), range(n_B)):\n",
    "        K[i, j] = np.sum(np.minimum(X[i], Y[j])) / np.sum(np.maximum(X[i], Y[j]))\n",
    "        \n",
    "    return K\n",
    "\n",
    "# Create some data\n",
    "___n_A = 45\n",
    "___n_B = 21\n",
    "___d = 121\n",
    "___X_A = np.random.RandomState(5943).randint(0, 43, size=(___n_A, ___d))\n",
    "___X_B = np.random.RandomState(842).randint(0, 43, size=(___n_B, ___d))\n",
    "\n",
    "# Symmetric kernel\n",
    "___K = minmax_kernel(___X_A)\n",
    "np.testing.assert_array_equal(___K, ___minmax_kernel_slow(___X_A))\n",
    "np.testing.assert_array_equal(np.diag(___K), np.ones((___n_A,)))\n",
    "assert(np.max(___K) <= 1.)\n",
    "assert(np.min(___K) >= 0.)\n",
    "\n",
    "# Non-symmetric kernel\n",
    "___K = minmax_kernel(___X_A, ___X_B)\n",
    "np.testing.assert_array_equal(___K, ___minmax_kernel_slow(___X_A, ___X_B))\n",
    "assert(np.max(___K) <= 1.)\n",
    "assert(np.min(___K) >= 0.)\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Toxicity Prediction using Non-Linear SVMs (1 Bonus Point) <a id='toxicity_prediction'></a>\n",
    "\n",
    "In this task you will predict whether a molecule can bind to a given receptor in the human body or not. Such prediction tasks do have relevance for drug design or environmental polution research. You are given a dataset with 600 molecular structures represented as molecular counting fingerprints (compare exercise 2), i.e. a non-negative vector where each entry counts the occurance of a predefined substructure in a molecule: \n",
    "\n",
    "\n",
    "![title](count_fingerprint_example.png \"Fingerprint example\")\n",
    "\n",
    "Let in the following $c(m_i)\\in\\mathbb{N}_{0}^d$ be the count vector representation of the molecule $m_i$. Furthermore, let $b(m_i)\\in\\{0,1\\}^d$ its binary representation, i.e. $b(m_i)_s = \\begin{cases} 1\\quad\\text{if }c(m_i)_s > 0 \\\\ 0\\quad\\text{else}\\end{cases}$. Depending on the kernel function we use, we define $\\mathbf{x}_i=c(m_i)$ respectively $\\mathbf{x}_i=b(m_i)$\n",
    "\n",
    "For each molecule you have the label $y_i\\in\\{-1,1\\}$ whether or whether not the molecules binds with the [aryl hydrocarbon receptor](https://en.wikipedia.org/wiki/Aryl_hydrocarbon_receptor). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_tox_data(idir=\"/coursedata/exercise03/toxicity/\", balance_classes=True,\n",
    "                  random_state=211, n_samples=600):\n",
    "    \"\"\"\n",
    "    Read in toxicity dataset.\n",
    "    \"\"\"        \n",
    "    smi_X = np.genfromtxt(idir + \"/maccs_count_nrahr.csv\", delimiter=\",\", comments=None, usecols=(0,), dtype=\"str\")\n",
    "    smi_y = np.genfromtxt(idir + \"/tox_nrahr.csv\", delimiter=\",\", comments=None, usecols=(0,), dtype=\"str\")\n",
    "\n",
    "    X = np.genfromtxt(idir + \"/maccs_count_nrahr.csv\", delimiter=\",\", comments=None)[:, 1:]\n",
    "    y = np.genfromtxt(idir + \"/tox_nrahr.csv\", delimiter=\",\", usecols=(1,), comments=None, dtype=\"int\")\n",
    "    y[y == 0] = -1\n",
    "\n",
    "    assert(np.all(smi_X == smi_y))\n",
    "    assert(len(np.unique(smi_X)) == len(smi_X))\n",
    "    \n",
    "    if balance_classes:\n",
    "        n_neg, n_pos = np.sum(y == -1), np.sum(y == 1)\n",
    "        idc_neg = np.random.RandomState(random_state).choice(n_neg, n_pos, replace=False)\n",
    "        \n",
    "        X_pos = X[y == 1]\n",
    "        X_neg = X[y == -1][idc_neg]\n",
    "        \n",
    "        y_pos = y[y == 1]\n",
    "        y_neg = y[y == -1][idc_neg]\n",
    "        \n",
    "        X, y = np.concatenate((X_pos, X_neg)), np.concatenate((y_pos, y_neg))\n",
    "        \n",
    "    # Get a random set of samples\n",
    "    rng = np.random.RandomState(random_state)\n",
    "    rnd_idx = rng.choice(X.shape[0], n_samples, replace=False)\n",
    "    \n",
    "    return X[rnd_idx], y[rnd_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in data\n",
    "X, y = read_tox_data()\n",
    "\n",
    "# Split into train and test\n",
    "X_train_c, X_test_c, y_train, y_test = train_test_split(X, y, random_state=3211)\n",
    "\n",
    "# Create binary version of count vector\n",
    "X_train_b, X_test_b = (X_train_c > 0).astype(\"float\"), (X_test_c > 0).astype(\"float\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <b>Tasks:</b>\n",
    "\n",
    "- Compare the performance of the Gaussian (rbf)-, MinMax- and Tanimoto Kernel (previous task) using on a test set.\n",
    "- Optimize the SVM hyper-parameters (and Gaussian-kernel parameters) using grid-search and 3-fold cross-validation.\n",
    "- Make use of the [sklearn C-SVM](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC) (imported as ```SVM_sk```) implementation (due to faster optimization).\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "    <b>Hints / Notes:</b> \n",
    "    \n",
    "- In this application the MinMax-kernel (for counting data) performs the best.\n",
    "- In the sklearn package, the gaussian kernel is called [RBF-kernel](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.rbf_kernel.html#sklearn.metrics.pairwise.rbf_kernel) and its parameter is $\\gamma.$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "toxicity_prediction",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Define the range of the hyper-parameters for the grid-search\n",
    "C_range = 2.**np.arange(-2, 5)\n",
    "gamma_range = np.array([0.001, 0.01, 0.1, 1.])\n",
    "\n",
    "# Define the random states for the cross-validation\n",
    "random_state_cv = 10909  # do not change!\n",
    "\n",
    "# Define a 3-fold cross-validation using the sklearn\n",
    "cv = None\n",
    "### BEGIN SOLUTION\n",
    "cv = KFold(n_splits=3, shuffle=True, random_state=random_state_cv)\n",
    "### END SOLUTION\n",
    "assert(cv.random_state == random_state_cv), \"Set the KFold random state.\"\n",
    "\n",
    "# Define 3 SVMs: using rbf-kernel, minmax-kernel and tanimoto kernel\n",
    "svm_gaus, svm_mm, svm_tan = None, None, None\n",
    "### BEGIN SOLUTION\n",
    "svm_gaus = SVM_sk(kernel=\"rbf\")\n",
    "svm_mm = SVM_sk(kernel=minmax_kernel)\n",
    "svm_tan = SVM_sk(kernel=tanimoto_kernel)\n",
    "### END SOLUTION\n",
    "\n",
    "# Define 3 GridSearchCV objects using the different SVMs\n",
    "est_gaus, est_mm, est_tan = None, None, None\n",
    "### BEGIN SOLUTION\n",
    "est_gaus = GridSearchCV(SVM_sk(kernel=\"rbf\"),\n",
    "                        param_grid={\"C\": C_range, \"gamma\": gamma_range}, cv=cv)\n",
    "est_mm = GridSearchCV(SVM_sk(kernel=minmax_kernel),\n",
    "                      param_grid={\"C\": C_range}, cv=cv)\n",
    "est_tan = GridSearchCV(SVM_sk(kernel=tanimoto_kernel), \n",
    "                       param_grid={\"C\": C_range}, cv=cv)\n",
    "### END SOLUTION\n",
    "\n",
    "# Fit the grid-search objects with the training data\n",
    "### BEGIN SOLUTION\n",
    "_ = est_gaus.fit(X_train_c, y_train)\n",
    "_ = est_mm.fit(X_train_c, y_train)\n",
    "_ = est_tan.fit(X_train_b, y_train)\n",
    "### END SOLUTION\n",
    "\n",
    "print(\"(RBF-kernel) score:\", np.round(est_gaus.score(X_test_c, y_test), 2), \n",
    "      \"best params:\", est_gaus.best_params_)\n",
    "print(\"(MinMax-kernel) score:\", np.round(est_mm.score(X_test_c, y_test), 2), \n",
    "      \"best params:\", est_mm.best_params_)\n",
    "print(\"(Tanimoto-kernel) score:\", np.round(est_tan.score(X_test_b, y_test), 2), \n",
    "      \"best_params:\", est_tan.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "toxicity_prediction_test",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### BEGIN HIDDEN TESTS\n",
    "np.testing.assert_allclose(est_gaus.score(X_test_c, y_test), 0.78)\n",
    "np.testing.assert_allclose(est_mm.score(X_test_c, y_test), 0.8266, atol=1e-4)\n",
    "np.testing.assert_allclose(est_tan.score(X_test_b, y_test), 0.7733, atol=1e-4)\n",
    "\n",
    "assert (est_gaus.best_params_ == {'C': 2.0, 'gamma': 0.01})\n",
    "assert (est_mm.best_params_ == {'C': 2.0})\n",
    "assert (est_tan.best_params_ == {'C': 4.0})\n",
    "### END HIDDEN TESTS"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
